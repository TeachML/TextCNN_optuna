{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install optuna datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import AdamW\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset, Dataset\n",
    "from collections import Counter\n",
    "from tqdm.cli import tqdm\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "from sklearn.metrics import PrecisionRecallDisplay, precision_recall_curve, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set torch device\n",
    "# mps device = Mac M1 and above\n",
    "device = torch.device(\"cpu\")\n",
    "if torch.has_cuda(): \n",
    "    device = torch.device(\"cuda\")\n",
    "if torch.has_mps():\n",
    "    device = torch.device(\"mps\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gender classification dataset\n",
    "dataset = load_dataset(\"mingiryu/name_gender_inference\")['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df = pd.DataFrame(dataset)\n",
    "dataset_df = dataset_df[dataset_df['gender'] != 'u']\n",
    "# coding labels, male = 0, female = 1\n",
    "dataset_df['label'] = dataset_df['gender'].apply(lambda ge: 0 if ge == 'm' else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(len(s) for s in dataset_df['full_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df.iloc[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a very simple char tokenizer with unknown token\n",
    "\n",
    "char_counter = Counter()\n",
    "for n in dataset_df['full_name']:\n",
    "    char_counter.update(n)\n",
    "\n",
    "char_dict = dict()\n",
    "char_dict['<pad>'] = 0\n",
    "char_dict['<unk>'] = 1\n",
    "char_dict['<s>'] = 2\n",
    "char_dict['</s>'] = 3\n",
    "\n",
    "# only keep chars with frequency >= 5\n",
    "for w in char_counter.keys():\n",
    "    if char_counter[w] < 5: \n",
    "        continue\n",
    "    char_dict[w] = len(char_dict)\n",
    "id_to_char = dict([(v, k) for (k, v) in char_dict.items()])\n",
    "\n",
    "# char_dict\n",
    "# id_to_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace unknown with <unk> in tokenizer\n",
    "# pad all names to max_length\n",
    "MAX_LENGTH = 30\n",
    "\n",
    "def tokenize_function(e):\n",
    "    tokenized_batch = dict()\n",
    "    tokenized_batch['input_ids'] = [\n",
    "        [char_dict.get(\"<s>\")] + \\\n",
    "        [char_dict.get(w, char_dict['<unk>']) for w in ee[:MAX_LENGTH]] + \\\n",
    "        [char_dict.get(\"</s>\")] +\n",
    "        [char_dict.get('<pad>')] * (MAX_LENGTH - len(ee)) \\\n",
    "        for ee in e['full_name']\n",
    "    ]\n",
    "    if 'label' in e:\n",
    "        tokenized_batch['label'] = e['label']\n",
    "\n",
    "    return tokenized_batch\n",
    "\n",
    "ds_all = Dataset.from_pandas(dataset_df[[\"full_name\", \"label\"]], preserve_index=False).map(tokenize_function, batched=True)\n",
    "\n",
    "ds_all = ds_all.remove_columns(['full_name'])\n",
    "\n",
    "for train_ids, test_ids in StratifiedShuffleSplit(n_splits=1, \n",
    "                                                  test_size=0.01, \n",
    "                                                  random_state=1331\n",
    "                                                  ).split(ds_all, ds_all['label']):\n",
    "    ds_train = Dataset.from_dict(ds_all[train_ids])\n",
    "    ds_test = Dataset.from_dict(ds_all[test_ids])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set format to torch tensors\n",
    "\n",
    "ds_train.set_format('torch')\n",
    "ds_test.set_format('torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, kernel_num, embed_dim, vocab_size, class_num):\n",
    "        super(TextCNN, self).__init__()\n",
    "        ci = 1  # input channel size\n",
    "        kernel_size = [2, 3, 4]\n",
    "        dropout = 0.5\n",
    "\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)        \n",
    "        self.convs = nn.ModuleList([nn.Conv2d(ci, kernel_num, (k, embed_dim)) for k in kernel_size])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc1 = nn.Linear(len(kernel_size) * kernel_num, class_num)\n",
    "\n",
    "        self.init_weight()\n",
    "        \n",
    "    def conv_and_pool(self, x, conv):\n",
    "        # x: (batch, 1, sentence_length, embed_dim)\n",
    "        x = conv(x)\n",
    "        # x: (batch, kernel_num, H_out, 1)\n",
    "        x = F.relu(x.squeeze(3))\n",
    "        # x: (batch, kernel_num, H_out)\n",
    "        x = F.max_pool1d(x, x.size(2)).squeeze(2)\n",
    "        #  (batch, kernel_num)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, sentence_length)\n",
    "        x = self.embed(x)\n",
    "        # x: (batch, sentence_length, embed_dim)\n",
    "        x = x.unsqueeze(1)\n",
    "        # x: (batch, 1, sentence_length, embed_dim)       \n",
    "        x = torch.cat([self.conv_and_pool(x, conv) for conv in self.convs], 1)  \n",
    "        # x: (batch, len(kernel_size) * kernel_num)\n",
    "        x = self.dropout(x)\n",
    "        logit = F.log_softmax(self.fc1(x), dim=1)\n",
    "        return logit\n",
    "\n",
    "    def init_weight(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 16\n",
    "test_batch_size = 16\n",
    "train_dataloader = torch.utils.data.DataLoader(ds_train, batch_size=train_batch_size, shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(ds_test, batch_size=test_batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_pt = torch.tensor([1., 1.]) # can set different weights to each class\n",
    "weight_pt = weight_pt.to(device)\n",
    "\n",
    "def train(model, device, train_dataloader, optim, epoch):\n",
    "    model.train()\n",
    "    total_training_data = len(train_dataloader.dataset)\n",
    "    total_batches = len(train_dataloader)\n",
    "    \n",
    "    optim.zero_grad()\n",
    "    \n",
    "    for b_i, b in enumerate(train_dataloader):\n",
    "        \n",
    "        X = b['input_ids']\n",
    "        y = b['label']\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        pred_prob = model(X)\n",
    "        loss = F.nll_loss(pred_prob, y, weight=weight_pt) # negative log-likelihood loss\n",
    "        loss.backward()\n",
    "        # gradient accumulation steps = 4\n",
    "        if (b_i % 4 == 0) or (b_i == total_batches - 1):\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "        # print log steps = 1000\n",
    "        if (b_i % 1000 == 0) or (b_i == total_batches - 1):\n",
    "            print('Epoch: {} [{}/{} ({:.0f}%)] training loss: {:.5f}'.format(\n",
    "                    epoch, \n",
    "                    (b_i + 1) * train_batch_size, \n",
    "                    total_training_data,\n",
    "                    100. * (b_i + 1) / total_batches, \n",
    "                    loss.item()\n",
    "                    ))\n",
    "\n",
    "def test(model, device, test_dataloader):\n",
    "    model.eval()\n",
    "    total_test_data = len(test_dataloader.dataset)\n",
    "\n",
    "    loss = 0.\n",
    "    success = 0.\n",
    "    with torch.no_grad():\n",
    "        for b in test_dataloader:\n",
    "            X = b['input_ids']\n",
    "            y = b['label']\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred_prob = model(X)\n",
    "            loss += F.nll_loss(pred_prob, y, reduction='sum').item()  # loss summed across the batch\n",
    "            pred = pred_prob.argmax(dim=1, keepdim=True)  # use argmax to get the most likely prediction\n",
    "            success += pred.eq(y.view_as(pred)).sum().item()\n",
    "\n",
    "    loss /= total_test_data\n",
    "    \n",
    "    accuracy = 100. * success / total_test_data\n",
    "\n",
    "    print('\\nTest set loss: {:.4f}, Accuracy: {:.0f}/{} ({:.0f}%)\\n'.format(\n",
    "        loss, \n",
    "        success, \n",
    "        total_test_data, \n",
    "        accuracy))\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "def objective(trial):\n",
    "    k_num = trial.suggest_int(\"kernel_num\", 128, 384, step=64) # output channel size\n",
    "    embed_dim = 128\n",
    "    # if you want to add another hyperparameter, comment the above and do so as follows\n",
    "    # embed_dim = trial.suggest_int(\"embed_dim\", 128, 384, step=64) # word embedding size\n",
    "    \n",
    "    model = TextCNN(kernel_num=k_num, embed_dim=embed_dim, vocab_size=len(char_dict), class_num=2)\n",
    "    model.to(device)\n",
    "    \n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-3, log=True)\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    \n",
    "    train_epochs = 3\n",
    "    for epoch in range(1, train_epochs + 1):\n",
    "        train(model, device, train_dataloader, optimizer, epoch)\n",
    "        accuracy = test(model, device, test_dataloader)\n",
    "        trial.report(accuracy, epoch)\n",
    "        \n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(study_name=\"gender_from_name\", direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=100, timeout=86400)\n",
    "\n",
    "pruned_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED]\n",
    "complete_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "\n",
    "print(\"results: \")\n",
    "print(\"num_trials_conducted: \", len(study.trials))\n",
    "print(\"num_trials_pruned: \", len(pruned_trials))\n",
    "print(\"num_trials_completed: \", len(complete_trials))\n",
    "\n",
    "print(\"results from best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"accuracy: \", trial.value)\n",
    "print(\"hyperparameters: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"{}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study.best_trial.params\n",
    "# {'kernel_num': 256, 'lr': 0.00044156544406933923}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use best params to do the final training\n",
    "\n",
    "model = None\n",
    "\n",
    "def final_train():\n",
    "    global model\n",
    "    kern_num = study.best_trial.params['kernel_num']\n",
    "    lr = study.best_trial.params['lr']\n",
    "    \n",
    "    model = TextCNN(kernel_num=kern_num, embed_dim=128, vocab_size=len(char_dict), class_num=2)\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    train_epochs = 3\n",
    "    for epoch in range(1, train_epochs + 1):\n",
    "        train(model, device, train_dataloader, optimizer, epoch)\n",
    "        test(model, device, test_dataloader)\n",
    "\n",
    "final_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"final_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when loading saved model for testing\n",
    "# kern_num = study.best_trial.params['kernel_num']\n",
    "# model = TextCNN(kernel_num=kern_num, embed_dim=128, vocab_size=len(char_dict), class_num=2)\n",
    "# model.load_state_dict(torch.load(\"final_model.pt\"))\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_test(model, device, test_dataloader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for b in test_dataloader:\n",
    "            X = b['input_ids']\n",
    "            X = X.to(device)\n",
    "            pred_prob = model(X)\n",
    "            pred_prob = torch.exp(pred_prob) # convert to probability\n",
    "            predictions.append(pred_prob.cpu())\n",
    "    predictions = torch.vstack(predictions)\n",
    "    predictions = predictions.numpy()\n",
    "    return predictions\n",
    "\n",
    "final_predictions = final_test(model, device, test_dataloader)\n",
    "display = PrecisionRecallDisplay.from_predictions(ds_test['label'].numpy(), final_predictions[:,1], name=\"TextCNN\")\n",
    "_ = display.ax_.set_title(\"Precision-Recall curve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = ['Male', 'Female']\n",
    "y_true = ds_test['label'].numpy()\n",
    "print(classification_report(y_true, torch.argmax(torch.tensor(final_predictions), dim=-1), target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_precision_recall_curve():\n",
    "    plt.rcParams['figure.figsize'] = [6, 4]\n",
    "    plt.rcParams['figure.dpi'] = 100 \n",
    "    plt.xlim([0.0, 1.2])\n",
    "    plt.ylim([0.0, 1.1])\n",
    "    plt.xticks([0.0, 0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "    plt.yticks([0.0, 0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "    plt.plot(precision, recall)\n",
    "    plt.title('Threshold / Precision-Recall Curve')\n",
    "    plt.xlabel('Precision')\n",
    "    plt.ylabel('Recall')\n",
    "    plt.grid(alpha=0.2)\n",
    "    last_y = 0.0\n",
    "    for i in range(0, len(precision) - 1):\n",
    "        if i == 0 or last_y - recall[i] > 0.05:\n",
    "            plt.text(precision[i] + 0.02,\n",
    "                     recall[i] + 0.02,\n",
    "                     \"{:.2f}\".format(thresholds[i]),\n",
    "                     alpha=0.8)\n",
    "            last_y = recall[i]\n",
    "            plt.plot(precision[i], recall[i], '-bo', markersize=3)\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_true, final_predictions[:, 1])\n",
    "\n",
    "visualise_precision_recall_curve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict helper\n",
    "\n",
    "def predict_one(text_in):\n",
    "\n",
    "    input_ids = tokenize_function({'full_name': [text_in.lower(),]})['input_ids']\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    input_ids = input_ids.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred_prob = model(input_ids)\n",
    "    pred_prob = torch.exp(pred_prob)\n",
    "    pred_prob = pred_prob.cpu().numpy()\n",
    "    return pred_prob[:, 1].item()\n",
    "\n",
    "predict_one(\"Aaaaaaaaa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_ROC_with_reference(fpr, tpr, thresholds_roc):\n",
    "    plt.rcParams['figure.figsize'] = [6, 4]\n",
    "    plt.rcParams['figure.dpi'] = 100 \n",
    "    plt.xlim([-0.2, 1.02])\n",
    "    plt.ylim([-0.01, 1.1])\n",
    "    plt.grid()\n",
    "    plt.plot(fpr, tpr, label='current classifier')\n",
    "\n",
    "    plt.xlabel('False Positive Rate / 1-Specificity')\n",
    "    plt.ylabel('True Positive Rate / Sensitivity')\n",
    "    last_y = 0.0\n",
    "    for i in range(1, len(tpr) - 1): \n",
    "         if i == 0 or tpr[i] - last_y >= 0.055:\n",
    "            plt.text(fpr[i] - 0.1,\n",
    "                     tpr[i] + 0.021,\n",
    "                     \"{:.2f}\".format(thresholds_roc[i]),\n",
    "                                     alpha=0.8)\n",
    "            last_y = tpr[i]\n",
    "            plt.plot(fpr[i],tpr[i], '-ro', markersize=3)\n",
    "    optimal_i = np.argmax(np.sqrt(tpr * (1 - fpr)))\n",
    "    print(\"Decision threshold: {:.2f}, tpr: {:.2f}, fpr: {:.2f}\"\n",
    "          .format(thresholds_roc[optimal_i], tpr[optimal_i], fpr[optimal_i]))\n",
    "    # random classifier curve\n",
    "    # plt.plot([0.0, 1.0], [0.0, 1.0], color='blue', linestyle='--', alpha=0.3, label=\"random classifier\")\n",
    "    # perfect classifier \n",
    "    plt.plot(0.0, 1.0, 'o', color='blue', label=\"perfect classifier\")\n",
    "    # most optimal threshold\n",
    "    plt.plot(fpr[optimal_i], tpr[optimal_i],' x', color='purple', markersize=12, label=\"threshold: {:2f}\".format(thresholds_roc[optimal_i]))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "fpr, tpr, thresholds_roc = roc_curve(y_true, final_predictions[:, 1])\n",
    "\n",
    "visualise_ROC_with_reference(fpr, tpr, thresholds_roc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set threshold as optimal and run eval\n",
    "thr = 0.00203\n",
    "\n",
    "female_prob = final_predictions[:,1]\n",
    "probs_after_threshold = [0 if rr < thr else 1 for rr in female_prob]\n",
    "\n",
    "cls_rep = classification_report(y_pred=probs_after_threshold,\n",
    "                                y_true=y_true,\n",
    "                                target_names=target_names, zero_division=0)\n",
    "print(cls_rep)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyt110",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "6fc7a988bd04ce138e7aac7b1bc3b3b09ac1e97a4c68f5f4e0f0293bedf6c695"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
